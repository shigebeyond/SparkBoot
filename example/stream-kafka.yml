# 单词统计： https://blog.csdn.net/weixin_46300771/article/details/123364722
- debug: true # 遇到df就show()
# 1 初始化spark session
- init_session:
    app: word-count
    # master: local[*]
    log_level: error # 日志级别
# 2 读kafka
- reads_kafka: # 字段是value
    lines:
        brokers: localhost:9092 # 多个用逗号分割
        topic: lines
# 3 查sql
- query_sql:
    words: select explode(split(value," ")) as word from lines
    word_count: select word, count(1) as cnt from words group by word
# 4 写kafka
- writes_kafka:
    word_count:
        brokers: localhost:9092
        topic: word_count
        outputMode: complete # append/update/complete
        checkpointLocation: ../data