- debug: true # 遇到df就show()
# 1 初始化spark session
- init_session:
    app: test
    master: local[*]
    log_level: error # 日志级别
  set_vars:
    outdir: ../data # 输出目录

# 2 读clickhouse
- read_jdbc:
    user:
      url: jdbc:clickhouse://192.168.62.209:9000/test
      table: user
      properties:
        user: root
        password: root
        # 需要提前复制好clickhouse驱动jar，参考pyspark.md
        #driver: ru.yandex.clickhouse.ClickHouseDriver
        driver: com.clickhouse.jdbc.ClickHouseDriver
# 3 写clickhouse
- write_clickhouse:
    user:
      url: jdbc:clickhouse://192.168.62.209:9000/test
      table: user
      properties:
        user: root
        password: root
        driver: com.clickhouse.jdbc.ClickHouseDriver
