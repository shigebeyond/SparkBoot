# 当 spark-submit 时记得修改env为cluster
- set_vars:
    env: local
    #env: cluster
- debug: true # 遇到df就show()
# 1 初始化spark session
- if(env=='local'):
  - init_session:
      app: test
      # master: local[*]
      log_level: error # 日志级别
    set_vars:
      outdir: ../data # 输出目录
  else:
  - init_session:
      app: test
      #master: spark://192.168.61.14:7077 # 不需要，spark-submit命令会指定
    set_vars:
      outdir: /output
# 2 读mysql
- read_jdbc:
    user:
      url: jdbc:mysql://192.168.61.14:3306/test
      table: user
      properties:
        user: root
        password: root
        driver: com.mysql.jdbc.Driver # 需要提前复制好mysql驱动jar，参考pyspark.md
# 3 写csv
- write_csv:
    #user: $outdir/user.csv
    user:
      path: $outdir/user.csv
      # 模式：append/overwrite/ignore
      mode: overwrite # 覆盖，否则写入报错 PATH_ALREADY_EXISTS
#      compression: none # 不压缩
# 4 读csv
- read_csv:
    user2:
        path: $outdir/user.csv
  print: "获得第1行的数据: ${user2[0][name]}"
# 5 查sql
- query_sql:
    age_count: select age, count(1) as n from user2 group by age # 按年龄分组统计人数
# 6 写csv
- write_csv:
    age_count:
      path: $outdir/age_count.csv
      mode: overwrite # 覆盖，否则写入报错 PATH_ALREADY_EXISTS
