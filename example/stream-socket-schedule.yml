# 单词统计： https://blog.csdn.net/weixin_46300771/article/details/123364722
# 命令行输入 nc -lk 9999
- debug: true # 遇到df就show()
# 1 初始化spark session
- init_session:
    app: word-count
    # master: local[*]
    log_level: error # 日志级别
# 2 读socket
- reads_socket: # 字段是value
    lines: localhost:9999
# 3 查sql
- query_sql:
    words: select explode(split(value," ")) as word, current_timestamp as ts from lines
    word_count: select word, count(1) as cnt from words group by word # 所有单词计数
# 4 写内存表, 表名为@word_count
- writes_mem:
    word_count:
        outputMode: complete # append/update/complete
        queryName: tmp_word_count
# 5 定时输出到console
- schedule(10):
    - write_console:
        # word_count: # AnalysisException: 'write' can not be called on streaming Dataset/DataFrame. -- 流只能输出到流(writeStream), 不能只输出一次(write) => 先存到临时内存表中
        tmp_word_count: